{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4853a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import errno\n",
    "import yaml\n",
    "\n",
    "# import functions file\n",
    "import sys\n",
    "file = 'etl.py'\n",
    "sys.path.insert(0,os.path.dirname(os.path.abspath(file)))\n",
    "import etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0719c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd099f25",
   "metadata": {},
   "source": [
    "### Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7276855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_et_file_name = 'config_et.yml'\n",
    "\n",
    "item_ADDITIONAL_INFORMATION_FILES = 'ADDITIONAL_INFORMATION_FILES'\n",
    "\n",
    "item_FILES_TO_PROCESS = 'FILES_TO_PROCESS'\n",
    "\n",
    "item_JOIN_FILES_COMMON_COLUMNS = 'JOIN_FILES_COMMON_COLUMNS'\n",
    "\n",
    "item_COLUMNS_TO_DROP = 'COLUMNS_TO_DROP'\n",
    "\n",
    "item_UPDATE_COLUMN_VALUES = 'UPDATE_COLUMN_VALUES'\n",
    "\n",
    "item_NEW_COLUMNS = 'NEW_COLUMNS'\n",
    "\n",
    "item_OUTPUT_FILE_NAME = 'OUTPUT_FILE_NAME'\n",
    "\n",
    "item_UPDATE_COLUMN_NAMES = 'UPDATE_COLUMN_NAMES'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cb91b9",
   "metadata": {},
   "source": [
    "### 1. Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3239bd1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "extract_and_join_\n",
    "files = etl.extract_and_join_files(config_et_file_name, item_FILES_TO_PROCESS, item_ADDITIONAL_INFORMATION_FILES, item_JOIN_FILES_COMMON_COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08a04f7",
   "metadata": {},
   "source": [
    "### 2. Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ab8a18",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "drop_not_used_columns = etl.drop_not_used_columns(config_et_file_name, item_COLUMNS_TO_DROP, extract_and_join_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f829336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_column_values = etl.update_column_values(config_et_file_name, item_UPDATE_COLUMN_VALUES, drop_not_used_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79a2bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_new_columns = etl.add_new_columns(config_et_file_name, item_NEW_COLUMNS, update_column_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e944ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "update_column_names = etl.update_column_names(config_et_file_name, item_UPDATE_COLUMN_NAMES, add_new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e00015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "etl.export_dataframe(config_et_file_name, item_OUTPUT_FILE_NAME, update_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dae046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bd4b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fn_graph import Composer\n",
    "\n",
    "composer = Composer().update(\n",
    "    list_of_variables,\n",
    "    extract_and_join_files\n",
    "    #drop_not_used_columns,\n",
    "    #update_column_values,\n",
    "    #add_new_columns,\n",
    "    #update_column_names,\n",
    "    #export_dataframe\n",
    ")\n",
    "\n",
    "composer.graphviz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ab3678",
   "metadata": {},
   "outputs": [],
   "source": [
    "composer.etl_extract_and_join_files()\n",
    "#composer.drop_not_used_columns()\n",
    "#composer.update_column_values()\n",
    "#composer.add_new_columns()\n",
    "#composer.update_column_names()\n",
    "#composer.export_dataframe()\n",
    "#composer.export_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c0f7fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5387c807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fn_graph import Composer\n",
    "\n",
    "import sys\n",
    "file = 'etl.py'\n",
    "sys.path.insert(0,os.path.dirname(os.path.abspath(file)))\n",
    "import etl\n",
    "    \n",
    "config_et_file_name = 'config_et.yml'\n",
    "\n",
    "item_ADDITIONAL_INFORMATION_FILES = 'ADDITIONAL_INFORMATION_FILES'\n",
    "\n",
    "item_FILES_TO_PROCESS = 'FILES_TO_PROCESS'\n",
    "\n",
    "item_JOIN_FILES_COMMON_COLUMNS = 'JOIN_FILES_COMMON_COLUMNS'\n",
    "\n",
    "item_COLUMNS_TO_DROP = 'COLUMNS_TO_DROP'\n",
    "\n",
    "item_UPDATE_COLUMN_VALUES = 'UPDATE_COLUMN_VALUES'\n",
    "\n",
    "item_NEW_COLUMNS = 'NEW_COLUMNS'\n",
    "\n",
    "item_OUTPUT_FILE_NAME = 'OUTPUT_FILE_NAME'\n",
    "\n",
    "item_UPDATE_COLUMN_NAMES = 'UPDATE_COLUMN_NAMES'\n",
    "    \n",
    "    \n",
    "def execute_extract_and_join_files(config_et_file_name, item_FILES_TO_PROCESS, \n",
    "                                      item_ADDITIONAL_INFORMATION_FILES, item_JOIN_FILES_COMMON_COLUMNS):\n",
    "    \n",
    "    \"\"\"\n",
    "    Execute \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return etl.extract_and_join_files(config_et_file_name, item_FILES_TO_PROCESS, item_ADDITIONAL_INFORMATION_FILES, item_JOIN_FILES_COMMON_COLUMNS)\n",
    "\n",
    "\n",
    "def execute_drop_not_used_columns(config_et_file_name, item_COLUMNS_TO_DROP, execute_extract_and_join_files):\n",
    "    \n",
    "    return etl.drop_not_used_columns(config_et_file_name, item_COLUMNS_TO_DROP, execute_extract_and_join_files)\n",
    "\n",
    "\n",
    "def execute_update_column_values(config_et_file_name, item_UPDATE_COLUMN_VALUES, execute_drop_not_used_columns):\n",
    "\n",
    "    return update_column_values(config_et_file_name, item_UPDATE_COLUMN_VALUES, execute_drop_not_used_columns)\n",
    "\n",
    "\n",
    "def execute_add_new_columns(config_et_file_name, item_NEW_COLUMNS, execute_update_column_values):\n",
    "\n",
    "    return etl.add_new_columns(config_et_file_name, item_NEW_COLUMNS, execute_update_column_values)\n",
    "\n",
    "\n",
    "def execute_update_column_names(config_et_file_name, item_UPDATE_COLUMN_NAMES, execute_add_new_columns):\n",
    "\n",
    "    return etl.update_column_names(config_et_file_name, item_UPDATE_COLUMN_NAMES, execute_add_new_columns)\n",
    "\n",
    "\n",
    "def export_dataframe(config_et_file_name, item_OUTPUT_FILE_NAME, execute_update_column_names):\n",
    "\n",
    "    etl.export_dataframe(config_et_file_name, item_OUTPUT_FILE_NAME, execute_update_column_names)\n",
    "\n",
    "\n",
    "composer = Composer().update(\n",
    "    execute_extract_and_join_files,\n",
    "    execute_drop_not_used_columns,\n",
    "    execute_update_column_values,\n",
    "    execute_add_new_columns,\n",
    "    execute_update_column_names,\n",
    "    export_dataframe    \n",
    ")\n",
    "\n",
    "composer.graphviz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f268333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fn_graph import Composer\n",
    "\n",
    "import sys\n",
    "file = 'etl.py'\n",
    "sys.path.insert(0,os.path.dirname(os.path.abspath(file)))\n",
    "import etl\n",
    "    \n",
    "config_et_file_name = 'config_et.yml'\n",
    "\n",
    "item_ADDITIONAL_INFORMATION_FILES = 'ADDITIONAL_INFORMATION_FILES'\n",
    "\n",
    "item_FILES_TO_PROCESS = 'FILES_TO_PROCESS'\n",
    "\n",
    "item_JOIN_FILES_COMMON_COLUMNS = 'JOIN_FILES_COMMON_COLUMNS'\n",
    "\n",
    "item_COLUMNS_TO_DROP = 'COLUMNS_TO_DROP'\n",
    "\n",
    "item_UPDATE_COLUMN_VALUES = 'UPDATE_COLUMN_VALUES'\n",
    "\n",
    "item_NEW_COLUMNS = 'NEW_COLUMNS'\n",
    "\n",
    "item_OUTPUT_FILE_NAME = 'OUTPUT_FILE_NAME'\n",
    "\n",
    "item_UPDATE_COLUMN_NAMES = 'UPDATE_COLUMN_NAMES'\n",
    "    \n",
    "    \n",
    "def execute_extract_and_join_files('config_et.yml', 'FILES_TO_PROCESS', 'ADDITIONAL_INFORMATION_FILES', 'JOIN_FILES_COMMON_COLUMNS'):\n",
    "\n",
    "    return etl.extract_and_join_files(config_et_file_name, item_FILES_TO_PROCESS, item_ADDITIONAL_INFORMATION_FILES, item_JOIN_FILES_COMMON_COLUMNS)\n",
    "\n",
    "\n",
    "def execute_drop_not_used_columns('config_et.yml', 'COLUMNS_TO_DROP', execute_extract_and_join_files):\n",
    "    \n",
    "    return etl.drop_not_used_columns(config_et_file_name, item_COLUMNS_TO_DROP, execute_extract_and_join_files)\n",
    "\n",
    "\n",
    "def execute_update_column_values('config_et.yml', 'UPDATE_COLUMN_VALUES', execute_drop_not_used_columns):\n",
    "\n",
    "    return update_column_values(config_et_file_name, item_UPDATE_COLUMN_VALUES, execute_drop_not_used_columns)\n",
    "\n",
    "\n",
    "def execute_add_new_columns('config_et.yml', 'NEW_COLUMNS', execute_update_column_values):\n",
    "\n",
    "    return etl.add_new_columns(config_et_file_name, item_NEW_COLUMNS, execute_update_column_values)\n",
    "\n",
    "\n",
    "def execute_update_column_names('config_et.yml', item_UPDATE_COLUMN_NAMES, execute_add_new_columns):\n",
    "\n",
    "    return etl.update_column_names(config_et_file_name, item_UPDATE_COLUMN_NAMES, execute_add_new_columns)\n",
    "\n",
    "\n",
    "def export_dataframe('config_et.yml', item_OUTPUT_FILE_NAME, execute_update_column_names):\n",
    "\n",
    "    etl.export_dataframe(config_et_file_name, item_OUTPUT_FILE_NAME, execute_update_column_names)\n",
    "\n",
    "\n",
    "composer = Composer().update(\n",
    "    execute_extract_and_join_files,\n",
    "    execute_drop_not_used_columns,\n",
    "    execute_update_column_values,\n",
    "    execute_add_new_columns,\n",
    "    execute_update_column_names,\n",
    "    export_dataframe    \n",
    ")\n",
    "\n",
    "composer.graphviz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4db0fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "composer.execute_extract_and_join_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a73ad81",
   "metadata": {},
   "source": [
    "### 3. Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b588247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import paramiko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c2b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_load_file_name = 'config_load.yml'\n",
    "item_DATABASE_CREDENTIALS = 'DATABASE_CREDENTIALS'\n",
    "item_FILE_TO_UPLOAD = 'FILE_TO_UPLOAD'\n",
    "item_NEW_TABLE_COLUMNS = 'NEW_TABLE_COLUMNS'\n",
    "item_TABLE_NAME = 'TABLE_NAME'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598350e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get database credentials (DATABASE_CREDENTIALS)\n",
    "\n",
    "# read information from config file\n",
    "database_credentials = get_yml_item_value(config_load_file_name, item_DATABASE_CREDENTIALS)\n",
    "\n",
    "# num to check if it is the firsts value of a key\n",
    "num = len(database_credentials)\n",
    "\n",
    "# get values of conection database into a string\n",
    "for key in database_credentials:\n",
    "    \n",
    "    if num == len(database_credentials):\n",
    "        \n",
    "        DB_CONNECTION_STRING = key.lower() + '=%s ' % database_credentials[key] \n",
    "    \n",
    "    else:\n",
    "        \n",
    "        DB_CONNECTION_STRING = DB_CONNECTION_STRING + key.lower() + '=%s ' % database_credentials[key] \n",
    "    \n",
    "    num -= 1\n",
    "\n",
    "# delete last empty element from connection string\n",
    "DB_CONNECTION_STRING = DB_CONNECTION_STRING.rstrip(DB_CONNECTION_STRING[-1])\n",
    "\n",
    "### Open database conection\n",
    "try: \n",
    "    conn = psycopg2.connect(DB_CONNECTION_STRING)\n",
    "    \n",
    "except psycopg2.Error as e: \n",
    "    print(\"Error: Could not make connection to the Postgres database\")\n",
    "    print(e)\n",
    "try: \n",
    "    cursor = conn.cursor()\n",
    "except psycopg2.Error as e: \n",
    "    print(\"Error: Could not get cursor\")\n",
    "    print(e)\n",
    "\n",
    "conn.set_session(autocommit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f8b608",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create new table in the database (NEW_TABLE_COLUMNS)\n",
    "\n",
    "# read columns to create new table from config file\n",
    "new_table_columns = get_yml_item_value(config_load_file_name, item_NEW_TABLE_COLUMNS)\n",
    "\n",
    "# get the table name\n",
    "table_name = get_yml_item_value(config_load_file_name, item_TABLE_NAME)[0].lower()\n",
    "\n",
    "# num to check if it is the firsts value of a key\n",
    "num = len(new_table_columns)\n",
    "\n",
    "# get values columns to build the sql statement to create the new table \n",
    "for key in new_table_columns:\n",
    "    \n",
    "    if num == len(new_table_columns):\n",
    "        \n",
    "        new_table_statement = ' \"\"\"CREATE TABLE IF NOT EXISTS ' + table_name + ' (' + key.lower() + ' %s,\\n' % new_table_columns[key] + '\\t'\n",
    "    \n",
    "    elif num == 1:\n",
    "        \n",
    "        new_table_statement = new_table_statement + key.lower() + ' %s,\\n' % new_table_columns[key] + ')\"\"\" ' \n",
    "    \n",
    "    else:\n",
    "        \n",
    "        new_table_statement = new_table_statement + key.lower() + ' %s,\\n' % new_table_columns[key] + '\\t'\n",
    "    \n",
    "    num -= 1\n",
    "\n",
    "new_table_statement = new_table_statement.rstrip(new_table_statement[-1])\n",
    "\n",
    "print(new_table_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e48219",
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~ Continuar aqui, ejecutar el sql statement ~~~~~~~~~~~~~~~~~~~~~~~~`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7ce9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get file to upload\n",
    "\n",
    "# get file to upload using the csv_file_to_df() function\n",
    "base_template = csv_file_to_df(config_load_file_name, item_FILE_TO_UPLOAD)\n",
    "base_template.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea22cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa97977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67ed577",
   "metadata": {},
   "outputs": [],
   "source": [
    "bitacoras_par = cursor.execute('''SELECT * FROM test;''')\n",
    "conn.commit()\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9dce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''SELECT * FROM information_schema.tables;'''\n",
    "  \n",
    "cursor.execute(sql)\n",
    "\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601f1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table_create = (\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS test(\n",
    "        id int PRIMARY KEY,\n",
    "        lai real NOT NULL);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10142cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_sql_statement(cursor, conn, sql_statement):\n",
    "### Function that executes the sql_staement\n",
    "\n",
    "    for query in sql_statement:\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            cursor.execute(query)\n",
    "            \n",
    "        except psycopg2.Error as e: \n",
    "            \n",
    "            print(\"Error: Issue executing SQL statement\")\n",
    "            \n",
    "            print(e)\n",
    "            \n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2521aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tables\n",
    "create_table_queries = [test_table_create]\n",
    "create_tables(cursor, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the database conection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8420b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd8ae12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "187d7f6b",
   "metadata": {},
   "source": [
    "### 3.2 Tested scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aedd307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete rows with values = 0\n",
    "lai_dataFrame = lai_dataFrame[lai_dataFrame['LAI'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1327eedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lai_dataFrame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcdf11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataFrame_to_csv(lai_dataFrame, 'LAI_ACRE_Biomass_y22')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c74e26",
   "metadata": {},
   "source": [
    "### 4. Test data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10263e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './output/LAI_ACRE_Biomass_y22.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e9dc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "date_early_stg = '2022-05-31'\n",
    "date_late_stg ='2022-06-09'\n",
    "\n",
    "date_early = pd.to_datetime(date_early_stg)\n",
    "date_late = pd.to_datetime(date_late_stg)\n",
    "\n",
    "df['DATE'] =  pd.to_datetime(df['DATE'])\n",
    "df['Days_after_planting'] = np.where(df['environment'] == 'Early', df['DATE'] - date_early, df['DATE'] - date_late) \n",
    "\n",
    "save_dataFrame_to_csv(df, 'LAI_ACRE_Biomass_y22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed2dee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter season\n",
    "    dff = df[df['season'] == season]\n",
    "    fig = px.box(dff, x=dff['LAI'], y=dff['plot'], color='environment', marginal=\"rug\", histfunc='count', labels={\n",
    "                     \"environment\": \"Date of planting\"}, hover_data=dff.columns, color_discrete_map = {'Late':'darkslategrey','Early':'gray'})\n",
    "\n",
    "    # strip down the rest of the plot\n",
    "    fig.update_layout(xaxis_title='Leaf area index ',\n",
    "                      yaxis_title='Frequency',\n",
    "                      plot_bgcolor='lavender',\n",
    "                      font_size=20,\n",
    "                      font_color='#000000',\n",
    "                      font_family='Old Standard TT')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a16188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfd767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fn_graph.examples.credit import composer\n",
    "\n",
    "book_locations = {\n",
    "    \"book_one\":\"s3://...\",\n",
    "    \"book_two\":\"s3://...\",\n",
    "}\n",
    "def loan_data(book_name):\n",
    "    return pd.read_csv(book_locations[book_name])\n",
    "\n",
    "production_composer = composer.update(\n",
    "    loan_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a6e2f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
