{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0880a08",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd226201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "import os\n",
    "import errno\n",
    "import yaml\n",
    "import time\n",
    "\n",
    "# import function files\n",
    "import sys\n",
    "file = 'etl_functions.py'\n",
    "sys.path.insert(0,os.path.dirname(os.path.abspath(file)))\n",
    "import etl_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd099f25",
   "metadata": {},
   "source": [
    "### Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7276855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_et_file_name = 'config_extract-transform.yml'\n",
    "item_ADDITIONAL_INFORMATION_FILES = 'ADDITIONAL_INFORMATION_FILES'\n",
    "item_FILES_TO_PROCESS = 'FILES_TO_PROCESS'\n",
    "item_JOIN_FILES_COMMON_COLUMNS = 'JOIN_FILES_COMMON_COLUMNS'\n",
    "item_COLUMNS_TO_DROP = 'COLUMNS_TO_DROP' \n",
    "item_UPDATE_ROW_VALUES = 'UPDATE_ROW_VALUES'\n",
    "item_NEW_COLUMNS = 'NEW_COLUMNS'\n",
    "item_OUTPUT_FILE_NAME = 'OUTPUT_FILE_NAME'\n",
    "item_UPDATE_COLUMN_NAMES = 'UPDATE_COLUMN_NAMES'\n",
    "item_UPDATE_PRIMARY_KEY_VALUES = 'UPDATE_PRIMARY_KEY_VALUES'\n",
    "item_PRIMARY_KEY_COLUMN = 'PRIMARY_KEY_COLUMN'\n",
    "item_CREATE_PRIMARY_KEY_IF_NEEDED = 'CREATE_PRIMARY_KEY_IF_NEEDED'\n",
    "item_TRANSPOSE_COLUMNS_TO_ONE_COLUMN = 'TRANSPOSE_COLUMNS_TO_ONE_COLUMN'\n",
    "item_NAMES_NEW_UNIQUE_COLUMNS = 'NAMES_NEW_UNIQUE_COLUMNS'\n",
    "\n",
    "# For deleting rows with invalid values \n",
    "additional_config_file = 'additional_configurations.yml'\n",
    "item_INVALID_VALUES = 'INVALID_VALUES'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cb91b9",
   "metadata": {},
   "source": [
    "### 1. Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973bec37",
   "metadata": {},
   "source": [
    "#### 1.1 Compare Dataframe column names \n",
    "Makes a comparison of the column names in all files in the _FILES_TO_PROCESS_ block of the **config_et.yml** file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0533ccd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files to compare: \n",
      "['example_file_to_process_width-height_1.csv', 'example_file_to_process_width-height_2.csv', 'example_file_to_process_width-height_3.csv', 'example_file_to_process_width-height_4.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 52.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output explanation: \n",
      " \n",
      " ([List columns on first file], \n",
      " [List columns NOT on first file])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['id_observation',\n",
       "  'observation_name',\n",
       "  'plot',\n",
       "  'rep',\n",
       "  'experiment',\n",
       "  'treatment',\n",
       "  'season',\n",
       "  'measurment',\n",
       "  'sampling_identifier',\n",
       "  'Height (cm)',\n",
       "  'Width (cm)',\n",
       "  'picture_Plot',\n",
       "  'picture_Experiment',\n",
       "  'Date Of Measurement',\n",
       "  'notes_Plot'],\n",
       " ['date', 'heigth (cm)', 'pictureofexperiment', 'pictureofplot'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etl_functions.compare_column_names(config_et_file_name , item_FILES_TO_PROCESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc6cd99",
   "metadata": {},
   "source": [
    "#### 1.2 Join data files and concatenate additional information \n",
    "Concatenates all files stored in the path specified in the FILES_TO_PROCESS block and then joins them with the additional information from files specified in the _ADDITIONAL_INFORMATION_FILES_ block using the columns specified at the _JOIN_FILES_COMMON_COLUMNS_ block of the **config_et.yml** file.\n",
    "\n",
    "**If the column of the additional information files and the data files do not have the same names, update the column names using the UPDATE_COLUMN_NAMES block.**\n",
    "\n",
    "Column names will be updated to lowercase and space replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92d7eb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files to process: \n",
      "['example_file_to_process_width-height_1.csv', 'example_file_to_process_width-height_2.csv', 'example_file_to_process_width-height_3.csv', 'example_file_to_process_width-height_4.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 60.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New dataframe info:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1400 entries, 0 to 1399\n",
      "Data columns (total 19 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   id_observation       1400 non-null   object \n",
      " 1   observation_name     1400 non-null   object \n",
      " 2   plot                 1400 non-null   int64  \n",
      " 3   rep                  1400 non-null   int64  \n",
      " 4   experiment_name      1400 non-null   object \n",
      " 5   treatment            1400 non-null   object \n",
      " 6   season               1400 non-null   object \n",
      " 7   measurment           1400 non-null   object \n",
      " 8   sampling_identifier  1400 non-null   object \n",
      " 9   height_(cm)          1397 non-null   float64\n",
      " 10  width_(cm)           1392 non-null   float64\n",
      " 11  picture_plot         7 non-null      object \n",
      " 12  picture_experiment   4 non-null      object \n",
      " 13  date                 1400 non-null   object \n",
      " 14  notes_plot           1 non-null      object \n",
      " 15  pictureofplot        1 non-null      object \n",
      " 16  pictureofexperiment  1 non-null      object \n",
      " 17  range                1400 non-null   int64  \n",
      " 18  entry                1400 non-null   int64  \n",
      "dtypes: float64(2), int64(4), object(13)\n",
      "memory usage: 207.9+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_and_join_files = etl_functions.extract_and_join_files(config_et_file_name, item_FILES_TO_PROCESS, item_UPDATE_COLUMN_NAMES, item_ADDITIONAL_INFORMATION_FILES, item_JOIN_FILES_COMMON_COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08a04f7",
   "metadata": {},
   "source": [
    "### 2. Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d30b4de",
   "metadata": {},
   "source": [
    "#### 2.1 Drop not desired columns\n",
    "Drop columns you do not want to keep in your final Dataframe using the column names specified in the _COLUMNS_TO_DROP_ of the **config_et.yml** file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74ab8a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to drop: ['pictureofplot', 'pictureofexperiment', 'picture_plot', 'picture_experiment', 'notes_plot', 'measurment']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping columns: 100%|████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 1995.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New dataframe info:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1400 entries, 0 to 1399\n",
      "Data columns (total 13 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   id_observation       1400 non-null   object \n",
      " 1   observation_name     1400 non-null   object \n",
      " 2   plot                 1400 non-null   int64  \n",
      " 3   rep                  1400 non-null   int64  \n",
      " 4   experiment_name      1400 non-null   object \n",
      " 5   treatment            1400 non-null   object \n",
      " 6   season               1400 non-null   object \n",
      " 7   sampling_identifier  1400 non-null   object \n",
      " 8   height_(cm)          1397 non-null   float64\n",
      " 9   width_(cm)           1392 non-null   float64\n",
      " 10  date                 1400 non-null   object \n",
      " 11  range                1400 non-null   int64  \n",
      " 12  entry                1400 non-null   int64  \n",
      "dtypes: float64(2), int64(4), object(7)\n",
      "memory usage: 142.3+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "drop_not_used_columns = etl_functions.drop_not_used_columns(config_et_file_name, item_COLUMNS_TO_DROP, extract_and_join_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d836325",
   "metadata": {},
   "source": [
    "#### 2. 2 Update column names \n",
    "Update column names according to what is specified in the _UPDATE_COLUMN_NAMES_ block of the **config_et.yml** file.\n",
    "\n",
    "Column names will be updated to lowercase and space replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cc28fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating column names: 100%|███████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 3283.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'experiment' NOT FOUND in the DataFrame.\n",
      "Column 'Date Of Measurement' NOT FOUND in the DataFrame.\n",
      "Column 'heigth (cm)' NOT FOUND in the DataFrame.\n",
      "\n",
      "New dataframe info:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1400 entries, 0 to 1399\n",
      "Data columns (total 13 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   date                 1400 non-null   object\n",
      " 1   entry                1400 non-null   object\n",
      " 2   experiment_name      1400 non-null   object\n",
      " 3   height_(cm)          1397 non-null   object\n",
      " 4   id_observation       1400 non-null   object\n",
      " 5   observation_name     1400 non-null   object\n",
      " 6   plot                 1400 non-null   object\n",
      " 7   range                1400 non-null   object\n",
      " 8   rep                  1400 non-null   object\n",
      " 9   sampling_identifier  1400 non-null   object\n",
      " 10  season               1400 non-null   object\n",
      " 11  treatment            1400 non-null   object\n",
      " 12  width_(cm)           1392 non-null   object\n",
      "dtypes: object(13)\n",
      "memory usage: 142.3+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "update_column_names = etl_functions.update_column_names(config_et_file_name, item_UPDATE_COLUMN_NAMES, drop_not_used_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac9fe21",
   "metadata": {},
   "source": [
    "#### 2.3 Insert new columns\n",
    "\n",
    "Insert new columns and fill their rows according to what was specified at the _NEW_COLUMNS_ block of the **config_et.yml** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55f2d742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding new columns: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 250.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following column was inserted: 'crop' : 'Wheat'\n",
      "\n",
      "New dataframe info:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1400 entries, 0 to 1399\n",
      "Data columns (total 14 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   date                 1400 non-null   object\n",
      " 1   entry                1400 non-null   object\n",
      " 2   experiment_name      1400 non-null   object\n",
      " 3   height_(cm)          1397 non-null   object\n",
      " 4   id_observation       1400 non-null   object\n",
      " 5   observation_name     1400 non-null   object\n",
      " 6   plot                 1400 non-null   object\n",
      " 7   range                1400 non-null   object\n",
      " 8   rep                  1400 non-null   object\n",
      " 9   sampling_identifier  1400 non-null   object\n",
      " 10  season               1400 non-null   object\n",
      " 11  treatment            1400 non-null   object\n",
      " 12  width_(cm)           1392 non-null   object\n",
      " 13  crop                 1400 non-null   object\n",
      "dtypes: object(14)\n",
      "memory usage: 153.3+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "add_new_columns = etl_functions.add_new_columns(config_et_file_name, item_NEW_COLUMNS, update_column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a08c7c",
   "metadata": {},
   "source": [
    "#### 2.4 Update row values\n",
    "Update the row values specified at the _UPDATE_ROW_VALUES_ block of the **config_et.yml** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eb1abdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating column values: 100%|███████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 177.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value 'a' was replaced by 'A'\n",
      "The value 'b' was replaced by 'B'\n",
      "The value 'c' was replaced by 'C'\n",
      "The value 'd' was replaced by 'D'\n",
      "The value 'Early' was replaced by 'Early planting date'\n",
      "The value 'Late' was replaced by 'Late planting date'\n",
      "The value 'ACRE-Biomass' was replaced by 'ACRE Public Biomass'\n",
      "The value 'y22' was replaced by 'Summer 2022'\n",
      "\n",
      "New dataframe head: \n",
      "\n",
      "        date entry      experiment_name height_(cm)  \\\n",
      "0  7/12/2022     3  ACRE Public Biomass       16.51   \n",
      "1  7/12/2022     3  ACRE Public Biomass       24.13   \n",
      "2  7/12/2022    24  ACRE Public Biomass       39.37   \n",
      "3  7/12/2022    24  ACRE Public Biomass       26.67   \n",
      "4  7/12/2022     7  ACRE Public Biomass        38.1   \n",
      "\n",
      "                                      id_observation observation_name plot  \\\n",
      "0  1_a_ACRE-Biomass_Early_y22_width-height_sampli...                A    1   \n",
      "1  1_b_ACRE-Biomass_Early_y22_width-height_sampli...                B    1   \n",
      "2  2_a_ACRE-Biomass_Early_y22_width-height_sampli...                A    2   \n",
      "3  2_b_ACRE-Biomass_Early_y22_width-height_sampli...                B    2   \n",
      "4  3_a_ACRE-Biomass_Early_y22_width-height_sampli...                A    3   \n",
      "\n",
      "  range rep sampling_identifier       season            treatment width_(cm)  \\\n",
      "0     2   1          sampling-1  Summer 2022  Early planting date      36.83   \n",
      "1     2   1          sampling-1  Summer 2022  Early planting date       38.1   \n",
      "2     2   1          sampling-1  Summer 2022  Early planting date       50.8   \n",
      "3     2   1          sampling-1  Summer 2022  Early planting date      45.72   \n",
      "4     2   1          sampling-1  Summer 2022  Early planting date      40.64   \n",
      "\n",
      "    crop  \n",
      "0  Wheat  \n",
      "1  Wheat  \n",
      "2  Wheat  \n",
      "3  Wheat  \n",
      "4  Wheat  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "update_column_values = etl_functions.update_row_values(config_et_file_name, item_UPDATE_ROW_VALUES, add_new_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2740dfd-9b6a-4f27-be7b-0c2c2bac80ea",
   "metadata": {},
   "source": [
    "#### 2.5 Transpose multiple columns to a single column\n",
    "Function that transposes the values from multiple columns to a single column, and it creates a column that specifies the measurement name and units of the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1658d742-b4ed-4a60-9c3b-d2eb90c28692",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming columns: 100%|█████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 363.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 empty VARIABLE VALUE ROWS were found in your data frame.\n",
      "\n",
      "EMPTY VARIABLE VALUE ROWS found were DELETED!\n",
      "\n",
      "New dataframe head: \n",
      "\n",
      "  observation_name sampling_identifier       season range       date entry  \\\n",
      "0                A          sampling-1  Summer 2022     2  7/12/2022     3   \n",
      "1                B          sampling-1  Summer 2022     2  7/12/2022     3   \n",
      "2                A          sampling-1  Summer 2022     2  7/12/2022    24   \n",
      "3                B          sampling-1  Summer 2022     2  7/12/2022    24   \n",
      "4                A          sampling-1  Summer 2022     2  7/12/2022     7   \n",
      "\n",
      "             treatment rep                                     id_observation  \\\n",
      "0  Early planting date   1  1_a_ACRE-Biomass_Early_y22_width-height_sampli...   \n",
      "1  Early planting date   1  1_b_ACRE-Biomass_Early_y22_width-height_sampli...   \n",
      "2  Early planting date   1  2_a_ACRE-Biomass_Early_y22_width-height_sampli...   \n",
      "3  Early planting date   1  2_b_ACRE-Biomass_Early_y22_width-height_sampli...   \n",
      "4  Early planting date   1  3_a_ACRE-Biomass_Early_y22_width-height_sampli...   \n",
      "\n",
      "       experiment_name plot   crop variable_value variable_units variable_name  \n",
      "0  ACRE Public Biomass    1  Wheat          36.83             cm         Width  \n",
      "1  ACRE Public Biomass    1  Wheat           38.1             cm         Width  \n",
      "2  ACRE Public Biomass    2  Wheat           50.8             cm         Width  \n",
      "3  ACRE Public Biomass    2  Wheat          45.72             cm         Width  \n",
      "4  ACRE Public Biomass    3  Wheat          40.64             cm         Width  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transpose_columns = etl_functions.transpose_multiple_columns_to_a_single_column(config_et_file_name, additional_config_file,  item_TRANSPOSE_COLUMNS_TO_ONE_COLUMN, \n",
    "                                              item_NAMES_NEW_UNIQUE_COLUMNS, item_INVALID_VALUES, update_column_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec41251",
   "metadata": {},
   "source": [
    "#### 2.6 Delete repeated rows and create a primary key if needed. \n",
    "\n",
    "Especially for files for data that were not collected using [AgTC](https://github.com/Purdue-LuisVargas/AgTC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5da3b0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No values provided for the CREATE_PRIMARY_KEY_IF_NEEDED block in the configuration file. No updates performed.\n",
      "Dataframe info: \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2789 entries, 0 to 1399\n",
      "Data columns (total 15 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   observation_name     2789 non-null   object\n",
      " 1   sampling_identifier  2789 non-null   object\n",
      " 2   season               2789 non-null   object\n",
      " 3   range                2789 non-null   object\n",
      " 4   date                 2789 non-null   object\n",
      " 5   entry                2789 non-null   object\n",
      " 6   treatment            2789 non-null   object\n",
      " 7   rep                  2789 non-null   object\n",
      " 8   id_observation       2789 non-null   object\n",
      " 9   experiment_name      2789 non-null   object\n",
      " 10  plot                 2789 non-null   object\n",
      " 11  crop                 2789 non-null   object\n",
      " 12  variable_value       2789 non-null   object\n",
      " 13  variable_units       2789 non-null   object\n",
      " 14  variable_name        2789 non-null   object\n",
      "dtypes: object(15)\n",
      "memory usage: 348.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "create_primary_key = etl_functions.delete_duplicate_and_create_primary_key(config_et_file_name, item_CREATE_PRIMARY_KEY_IF_NEEDED, \n",
    "                                                                item_PRIMARY_KEY_COLUMN, transpose_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43da37c",
   "metadata": {},
   "source": [
    "#### 2.7 Update primary key values\n",
    "Function that updates some characters of the primary key string. It is useful when more than one trait is collected using the same template.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8cedf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating key: 100%|█████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 659.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataframe head: \n",
      "\n",
      "  observation_name sampling_identifier       season range       date entry  \\\n",
      "0                A          sampling-1  Summer 2022     2  7/12/2022     3   \n",
      "1                B          sampling-1  Summer 2022     2  7/12/2022     3   \n",
      "2                A          sampling-1  Summer 2022     2  7/12/2022    24   \n",
      "3                B          sampling-1  Summer 2022     2  7/12/2022    24   \n",
      "4                A          sampling-1  Summer 2022     2  7/12/2022     7   \n",
      "\n",
      "             treatment rep                                   id_observation  \\\n",
      "0  Early planting date   1  1_a_ACRE-Biomass_Early_y22_height_samplinging-1   \n",
      "1  Early planting date   1  1_b_ACRE-Biomass_Early_y22_height_samplinging-1   \n",
      "2  Early planting date   1  2_a_ACRE-Biomass_Early_y22_height_samplinging-1   \n",
      "3  Early planting date   1  2_b_ACRE-Biomass_Early_y22_height_samplinging-1   \n",
      "4  Early planting date   1  3_a_ACRE-Biomass_Early_y22_height_samplinging-1   \n",
      "\n",
      "       experiment_name plot   crop variable_value variable_units variable_name  \n",
      "0  ACRE Public Biomass    1  Wheat          36.83             cm         Width  \n",
      "1  ACRE Public Biomass    1  Wheat           38.1             cm         Width  \n",
      "2  ACRE Public Biomass    2  Wheat           50.8             cm         Width  \n",
      "3  ACRE Public Biomass    2  Wheat          45.72             cm         Width  \n",
      "4  ACRE Public Biomass    3  Wheat          40.64             cm         Width  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "update_primary_key_values = etl_functions.update_primary_key_values(config_et_file_name, item_UPDATE_PRIMARY_KEY_VALUES, \n",
    "                                                                    item_PRIMARY_KEY_COLUMN, create_primary_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336980f0",
   "metadata": {},
   "source": [
    "#### 2.8 Export the final data frame \n",
    "Export the final data frame to a location specified at the _OUTPUT_FILE_NAME_ block of the **config_et.yml** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e00015c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe: ./et_output/20240516-124816_example_output_file.csv created successfully!\n"
     ]
    }
   ],
   "source": [
    "etl_functions.export_dataframe(config_et_file_name, item_OUTPUT_FILE_NAME, update_primary_key_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad8d6ad-3529-4900-abeb-138a69c91444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
